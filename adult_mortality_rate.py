# -*- coding: utf-8 -*-
"""Adult_Mortality_Rate

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/adult-mortality-rate-dae58ce8-d41c-4bdf-98e0-2a2c78fb9ddc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240405/auto/storage/goog4_request%26X-Goog-Date%3D20240405T035847Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47c3662bfa7b087ced1be396f413ce71bc5317ae816d59325a62335b578bb0b33980127ab5bd0f6426fe3ba9a754235d2cb744b0fc0512e007c33cf31a0948ba0530f2d325f38b250a1c171e77b834ac00f164b2871fcdcc3e3380e57a06c5baaa87bf0552f15a6b17e1c28ac7a5938686f5cd1577d0d904e5116e8a277e16aa250165cd7d7a4ab3cc78ec86d41f596cbbcfab1347c7d1d190de6c5aa7bac945ee7100897e976aed8c0ee7e8bce52855211fe77bbddc347ce10d8a88089150e13a99a3e76d97dfb26387c97f0d87b53a1e0d2d2acce8dc06115da7e7aa35913833b456c1edff3eafc45d42c307b842500999d1042cb93c488b76f3ce0616b336
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'adult-mortality-rate-2019-2021:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4645614%2F7990801%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240405%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240405T035847Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D29ca80e4ca9208dfd73775f742e75c5eb6531f913875600f315bfd92d2527cc26ab1d36a2876644c03548a27a5754423361352c00775cf3d36f92929aa69d96fb49efb6f6bfa62f6dd49f94c586f5306b8cc0a3de8b6c5ae3e44f7462d4933e81c8dfa080913657ceb1755c3b0d40c4c04a051ab4783f330ba34838cfd9009ed99e6011e520ee53fbbddf30b03408d2dd10e78476edf3ae8b7cdcfe3decffcd6c39ee52ffcfe3b354ffefa9797158c4949fb7833033e60d18d1a7a6aed7161e95048a83c4ecc98324b7a7012334a5b75e62383b1903638e56c66076c627614841e2da1f92e1f6bc2a97079fcfefce116f69085e4715b16d95037e3ce94ffcd5f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""<a id="title"></a>
<h1 style='font-family: arial; background-color:#d7cec7; font-size:350%; color:black; border-radius: 50px 50px;
           text-align: center; border-bottom: 5px solid #c09f80;
           padding-left: 20px; padding-right: 20px; padding-bottom: 20px; padding-top: 20px;
           '>Adult Mortality Rate Analysis</h1>

<a id='goal'></a>
<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Goal</p>
<p style="font-size: 16px; font-family:arial; text-align: justify;">
The goal of this project is to use various factors to predict Average crude mortality rate in individuals (2019-2021)</p>

<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">
Dataset Descriptionüí°</p>
<p style='font-size: 16px; font-family:arial; text-align: justify;'>The <a href="https://www.kaggle.com/datasets/priyanshusethi/happiness-classification-dataset">dataset</a> contains information on mortality rates in different countries of the world and some factors that may affect this rate for 2019-2023.
</p>

|Feature|	Description|
|:---|:---|
|Countries|	Country of study.|
|Continent|	Continent location of the country.|
|Average_Pop (thousands people)|	Average population of the country under study for 2019-2021 in thousands.|
|Average_GDP (Miliion dollar)|	Average GDP of the country under study for 2019-2021 in millions of dollars.|
|Average_GDP_per_capita|	Average GDP per capita of the country under study for 2019-2021 in dollars.|
|Average_HEXP (dollar)|	Health Expenditure Per Capita in the country under study in dollars.|
|Development_level|	Level of development of the state under study (calculated by GDP per capita of the country).|
|AMR_female (per 1000 female adults)|	Average mortality of adult women in the country under study (per 1000 adult women per year) for 2019-2023.|
|AMR_male (per 1000 male adults)|	Average mortality of adult men in the country under study (per 1000 adult men per year) for 2019-2023.|
|Average_CDR|	Average crude mortality rate for 2019‚Äì2021 in the country under study.|

<p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">
Acknowedgementüí°</p>

<p style='font-size: 16px; font-family:arial; text-align: justify;'>
Big thanks to the dataset owner, <a href=https://www.kaggle.com/mikhail1681>Mikhail</a>. Thank you for your dedication and hardwork!</p>

<left>
    <p style="font-size: 18px; font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;"><b>
        Table of Contents</b></p>
    <ol id='contents' style='font-family: arial; font-size: 16px'>
    <li><a href='#goal'>Goal</a></li>
    <li><a href='#library'>Libraries &amp Dataset</a></li>
    <li><a href='#info'>Basic Information</a></li>
    <li><a href='#train-test'>Train &amp Test</a></li>
    <li><a href='#eda'>EDA</a></li>
    <li><a href='#fe'>Feature Engineering</a></li>
    <li><a href='#encoding'>Encoding</a></li>
    <li><a href='#preprocessing'>Preprocessing</a></li>
    <li><a href='#base_models'>Base Models</a></li>
    <li><a href='#tuning'>Tuning with Optuna</a></li>
    <li><a href='#shap'>Model Explainability</a></li>
</ol>

<a id='library'></a>
<h2 style=" font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Libraries &amp Dataset</b><a href='#contents'> üìñ</a></h2>
"""

import os, time, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import RobustScaler, FunctionTransformer, OrdinalEncoder, OneHotEncoder
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error, make_scorer
from sklearn.pipeline import Pipeline

from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner

import shap

## show all available matplotlib style
# plt.style.available

## Set seed, style and source url
SEED = 91
plt.style.use('seaborn-v0_8-bright')
SOURCE = "/kaggle/input/adult-mortality-rate-2019-2021/Adult mortality rate (2019-2021).csv"

"""<a id='info'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Basic Information<a href='#contents'> üîç</a></h2>
"""

## load data & show samples
df = pd.read_csv(SOURCE)
df.head(5)

## basic info showing data types, missing values
df.info()

## check number of duplicate rows
df.duplicated(keep='first').sum()

## check missing values by column
df.isnull().sum(axis=0)

## descriptive statistics
df.describe()

## split to categorical table and numerical table
num_col = df.select_dtypes(exclude='object')
cat_col = df.select_dtypes(include='object')
cat_col["Average_CDR"] = df["Average_CDR"]
print(num_col.shape)
print(cat_col.shape)

## trying to identify 0 and inf
num_col.describe().loc[["min","max"],:].T

## identify unique values and how frequent they appear
cat_col.iloc[:,:-1].describe()

"""> <p style='font-family: arial; font-size:16px; text-align: justify;padding-left: 20px; padding-right: 20px; padding-bottom: 20px; padding-top: 20px;'>All values in Countries feature is text but unique, which does not provide much information. We can drop this feature.</p>"""

## identify outliers
df[np.abs(stats.zscore(df.iloc[:,-1]))> 3]

"""<a id='train-test'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Train &amp Test<a href='#contents'> ‚úÇ</a></h2>
"""

X_train, X_test, y_train, y_test = train_test_split(df.drop("Average_CDR",axis=1),
                                                    df["Average_CDR"],
                                                    test_size=0.1,
                                                    random_state=SEED,
                                                    stratify=df["Development_level"])

print(f"Training set has {X_train.shape[0]} rows and {X_train.shape[1]+y_train.ndim} columns")
print(f"Test set has {X_test.shape[0]} rows and {X_test.shape[1]+y_test.ndim} columns")

## drop outliers
print(X_train.shape)
outlier_index = X_train[np.abs(stats.zscore(X_train.iloc[:,-1]))> 3].index.tolist()
X_train.drop(outlier_index,axis=0,inplace=True)
print(X_train.shape)
y_train.drop(outlier_index,axis=0,inplace=True)

"""<a id='eda'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">EDA <a href='#contents'>üìà</a></h2>
"""

train_set = X_train.merge(right=y_train,how='inner',left_index=True,right_index=True)
train_set.info()

def show_box_plot(df):
    df = df.select_dtypes(exclude='object')
    ncol = 2
    nrow = df.shape[1]
    height = nrow*4
    fig, ax = plt.subplots(nrow, ncol, figsize = (12,height))
    i = 0
    for col in df.columns:
        sns.boxplot(data=df,x=col,ax=ax[i][0], color="#861D21")
        sns.boxplot(data=np.log1p(df),x=col,ax=ax[i][1], color="#861D21")
        ax[0][0].set_title("Original")
        ax[0][1].set_title("Log transform")
        i += 1
    plt.tight_layout()

show_box_plot(train_set)

def show_unique_category(df):
    warnings.filterwarnings('ignore')
    df = df.select_dtypes(include='object')
    ncol = 3
    nrow = 1
    height = nrow*5
    fig, ax = plt.subplots(nrow, ncol, figsize = (12,height))
    ax = ax.flatten()
    i = 0
    for col in df.columns:
        if col not in ["Countries"]:
            temp = df[col].value_counts()
            ax[i].bar(x=temp.index,height=temp.values, color="#861D21")
            ax[i].set_title(col+" count plot")
            if col == 'Continent_dev':
                for tick in ax[i].get_xticklabels():
                    tick.set_rotation(90)
            else:
                for tick in ax[i].get_xticklabels():
                    tick.set_rotation(25)
            for j in range(len(temp)):
                ax[i].text(j,temp[j],temp[j])
            i+=1

    plt.tight_layout()

show_unique_category(train_set)

def show_target_by_category(df,target,row=2,col=1):
    warnings.filterwarnings("ignore")
    t = df[target]
    df = df.select_dtypes(include=['object'])
    df[target] = t
    ncol = col
    nrow = row
    height = nrow*4
    fig, ax = plt.subplots(nrow, ncol, figsize = (12,height))
    ax = ax.flatten()
    i = 0
    for col in df.columns:
        if col not in ["Countries",target]:
            t = np.round(df[[col,target]].groupby(col).agg("mean")[target],2).sort_values(ascending=False)
#             ax[i].pie(t,labels=list(zip(t.index,t.values)))
            ax[i].bar(x=t.index,height=t.values,color="#861D21")
            if col == 'Continent_dev':
                for tick in ax[i].get_xticklabels():
                    tick.set_rotation(45)
            ax[i].set_title("Mean of "+target+"\nby "+col,size=12)
            i+=1
    plt.tight_layout()

show_target_by_category(train_set,"Average_CDR")

"""> <p style='font-family: arial;'>The plot above gives an idea of using ordinal encoding for Continent and one-hot encoding for Development_level</p>"""

def show_scatter(df, target, row=2, col=3):
    warnings.filterwarnings("ignore")
    df = df.select_dtypes(exclude='object')
    ncol = col
    nrow = row
    height = nrow*3.5
    fig, ax = plt.subplots(nrow, ncol, figsize = (12,height))
    ax = ax.flatten()
    i = 0
    for col in df.columns[:-1]:
        sns.regplot(data=df,x=col,y=target,ax=ax[i],color="#861D21")
        i += 1
    plt.tight_layout()

show_scatter(train_set,"Average_CDR")

sns.regplot(x=(X_train["AMR_male(per_1000_male_adults)"]/X_train["AMR_female(per_1000_female_adults)"]),
                y=y_train)

sns.regplot(x=(X_train["Average_HEXP($)"]/X_train["Average_GDP_per_capita($)"]),
                y=y_train)

df.columns

df.sort_values(by='Average_GDP_per_capita($)',ascending=False).head(20)

def show_correlation(df, method='pearson'):
    df = df.select_dtypes(exclude='object')
    corr = df.corr(method=method)
    mask = np.triu(corr)
    sns.heatmap(corr,mask=mask,annot=True,cmap='Oranges',fmt='.2f')

show_correlation(train_set,'pearson')

def show_distribution(df):
    warnings.filterwarnings("ignore")
    df = df.select_dtypes(exclude='object')
    ncol = 2
#     nrow = [(df.shape[1] // 3) + 1 if df.shape[1]%3 != 0 else df.shape[1]//3][0]
    nrow = df.shape[1]
    height = nrow*4
    fig, ax = plt.subplots(nrow, ncol, figsize = (12,height))
#     ax = ax.flatten()
    i = 0
    for col in df.columns:
        sns.histplot(data=df,x=col,kde=True,ax=ax[i][0])
        sns.histplot(data=np.log1p(df),x=col,kde=True,ax=ax[i][1])
        ax[0][0].set_title("Original")
        ax[0][1].set_title("Log transform")
        i += 1

show_distribution(train_set)

"""<a id='fe'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Feature Engineering <a href='#contents'> ü§Ø</a></h2>
"""

NEW_FEATURE = True

X_train.head(3)

if NEW_FEATURE:
    ## "Average_HEXP($)" is per capita so we are using GDP per capita
    X_train["HEXP_GDP_ratio"] = X_train["Average_HEXP($)"]/X_train["Average_GDP_per_capita($)"]
    X_test["HEXP_GDP_ratio"] = X_test["Average_HEXP($)"]/X_test["Average_GDP_per_capita($)"]

    ## ratio of mortality rate by gender
    X_train["ratio_AMR"] = X_train["AMR_male(per_1000_male_adults)"]/X_train["AMR_female(per_1000_female_adults)"]
    X_test["ratio_AMR"] = X_test["AMR_male(per_1000_male_adults)"]/X_test["AMR_female(per_1000_female_adults)"]

    num_col = X_train.select_dtypes(exclude='object')
    cat_col = X_train.select_dtypes(include='object')
    cat_col["Average_CDR"] = y_train

num_col.describe().loc[["min","max"],:]

cat_col.iloc[:,:-1].describe()

show_scatter(train_set,"Average_CDR")

sns.regplot(x=(X_train["AMR_male(per_1000_male_adults)"]/X_train["AMR_female(per_1000_female_adults)"]),
                y=y_train)

sns.regplot(x=(X_train["Average_HEXP($)"]/X_train["Average_GDP_per_capita($)"]),
                y=y_train)

DROP = False

if DROP:
    try:
        col_drop = [
                      "Average_Pop(thousands people)","Average_GDP(M$)",
                      "Average_GDP_per_capita($)","Average_HEXP($)",
                      "AMR_male(per_1000_male_adults)",
                      "AMR_female(per_1000_female_adults)"
                   ]

        X_train.drop(col_drop, axis=1,inplace=True)
        X_test.drop(col_drop, axis=1,inplace=True)
        print("Drop completed")

    except:
        print("Already dropped.")

"""<a id='encoding'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Encoding <a href='#contents'> üë®üèª‚Äçüíª</a></h2>
"""

## encoders preparation
continent_encoder = {
    "Asia": 1,
    "Australia": 2,
    "North America": 3,
    "South America": 4,
    "Africa": 5,
    "Europe":6
}

development_encoder = OneHotEncoder(dtype=int,
                                    handle_unknown="ignore",
                                    drop='first',
                                    sparse_output=False).set_output(transform='pandas')
development_encoder.fit(X_train[["Development_level"]])

X_train.drop("Countries",axis=1,inplace=True) #,'Average_Pop(thousands people)','Average_GDP(M$)'
X_test.drop("Countries",axis=1,inplace=True)

X_train_2 = X_train.copy()
X_test_2 = X_test.copy()

dummy_train = development_encoder.transform(X_train[["Development_level"]])
dummy_test = development_encoder.transform(X_test[["Development_level"]])

X_train_2["Continent"] = X_train_2["Continent"].apply(lambda x: continent_encoder[x])
X_test_2["Continent"] = X_test_2["Continent"].apply(lambda x: continent_encoder[x])

X_train_2 = pd.concat([X_train_2,dummy_train],
                      axis=1,join='inner')
X_test_2 = pd.concat([X_test_2,dummy_test],
                      axis=1,join='inner')

try:
    X_train_2.drop(["Development_level"],axis=1,inplace=True)
    X_test_2.drop(["Development_level"],axis=1,inplace=True)
    print("Completed.")
except:
    print("Already dropped.")

X_train_2.head()

X_test_2.head()

"""<a id='preprocessing'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Preprocessing <a href='#contents'>ü§ì</a></h2>
"""

fx_transformer = FunctionTransformer(np.log1p)

for col in X_train_2.columns[1:-2]:
    X_train_2[col] = fx_transformer.transform(X_train_2[col])
    X_test_2[col] = fx_transformer.transform(X_test_2[col])

"""<a id='base_models'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Base Models <a href='#contents'>ü§ñ</a></h2>
"""

reg_dict = {
    "linear": LinearRegression(),
    "lasso": Lasso(random_state=SEED),
    "ridge": Ridge(random_state=SEED),
    "svr": SVR(),
    "tree": DecisionTreeRegressor(random_state=SEED),
    "forest": RandomForestRegressor(random_state=SEED),
    "hist": HistGradientBoostingRegressor(random_state=SEED),
    "lgbm": LGBMRegressor(verbose=-1,random_state=SEED),
    "xgb": XGBRegressor(random_state=SEED),
    "cat": CatBoostRegressor(random_state=SEED, verbose=False,cat_features=['Continent'])
}

def train_baselines(X_train,y_train, reg_dict):
    scaler = RobustScaler()
    model_name = []
    cv_scores = []
    cv_std = []
    start_time = time.time()

    for model in reg_dict.keys():
        if model != "cat":
            reg_pipeline = Pipeline([("scaler",scaler),("reg",reg_dict[model])])
            cv = cross_val_score(reg_pipeline,X_train, y_train, cv=4,n_jobs=os.cpu_count(),scoring='r2')

        else:
            reg_pipeline = Pipeline([("scaler",None),("reg",reg_dict[model])])
            cv = cross_val_score(reg_pipeline,X_train, y_train, cv=4,n_jobs=os.cpu_count(),scoring='r2')

        model_name.append(model)
        cv_scores.append(cv.mean())
        cv_std.append(cv.std(ddof=0))

    cv_result = pd.DataFrame(np.column_stack((model_name,cv_scores,cv_std)),columns=['model','score','stddev'])
    finish_time = time.time()
    print(f"Training completed in {finish_time-start_time:.2f} seconds.")
    return cv_result

cv_result = train_baselines(X_train_2,y_train,reg_dict)

cv_result["score"]= np.round(cv_result["score"].astype('float32'),3)
cv_result["stddev"]= np.round(cv_result["stddev"].astype('float32'),3)

cv_result.sort_values(by='score',ascending=False,inplace=True)
ax = cv_result.plot(kind='bar',y=["score","stddev"],x="model",title='Average CV score by model')
ax.bar_label(ax.containers[0]);
ax.bar_label(ax.containers[1]);

## try with test set
test_set_score = []
for model in reg_dict.keys():
    reg = reg_dict[model]
    reg.fit(X_train_2,y_train)
    y_pred = reg.predict(X_test_2)
    score = r2_score(y_test,y_pred)
    test_set_score.append([model,score])

test_set_score = pd.DataFrame(test_set_score,columns=["model","test_score"]).sort_values(by='test_score',ascending=False)
ax = test_set_score.plot(kind='bar',y="test_score",x="model",title='Test set score by model')
ax.bar_label(ax.containers[0]);



reg = reg_dict['cat']
reg.fit(X_train_2,y_train)

def residual_plot(reg,X_train,y_train,X_test,y_test):
    fig, ax = plt.subplots(2,2,figsize=(11,8))
    ax = ax.flatten()
    y_pred_1 = reg.predict(X_train)
    res_1 = y_train - y_pred_1
    y_pred_2 = reg.predict(X_test)
    res_2 = y_test - y_pred_2



    sns.regplot(x=y_pred_1,y=y_train,color='green',ax=ax[0])
    ax[0].set_xlabel("y_pred")
    ax[0].set_ylabel("y_train")

    sns.regplot(x=y_pred_2,y=y_test,color='red',ax=ax[1])
    ax[1].set_xlabel("y_pred")
    ax[1].set_ylabel("y_test")

    sns.regplot(x=y_pred_1,y=(res_1-res_1.mean())/res_1.std(),color='green',ax=ax[2], ci=None)
    ax[2].set_xlabel("y_pred (train)")
    ax[2].set_ylabel("standardized residual")

    sns.regplot(x=y_pred_2,y=(res_2-res_2.mean())/res_2.std(),color='red',ax=ax[3],ci=None)
    ax[3].set_xlabel("y_pred (test)")
    ax[3].set_ylabel("standardized residual")

residual_plot(reg,X_train_2,y_train,X_test_2,y_test)

feature_importance = pd.DataFrame(reg.feature_importances_,
                                  columns=["importance"],
                                  index=reg.feature_names_)
feature_importance["importance"] = feature_importance["importance"].astype('float32')

ax = feature_importance.sort_values(by='importance',ascending=False).plot(kind='bar')
ax.bar_label(ax.containers[0]);

"""[Source](https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/)

<a id='tuning'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Hyperparameter tuning with Optuna <a href='#contents'>‚¨Ü</a></h2>
"""

TUNE = True

# Catboost tuning
if TUNE:
    start_time = time.time()
    scorer = make_scorer(r2_score, greater_is_better=True)

    def objective(trial):
        params = {
            "iterations": trial.suggest_int("iterations", 100, 2000),
            "learning_rate": trial.suggest_float("learning_rate",1e-4,0.1),
            "depth": trial.suggest_int("depth", 1, 9),
            "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.1, 1.0),
            "verbose": False,
            "cat_features":['Continent'],
            "random_seed": SEED
        }
        reg = CatBoostRegressor(**params)
        scores = cross_val_score(reg, X_train_2, y_train, scoring=scorer, cv=4, n_jobs=os.cpu_count(), verbose=2)

        return np.mean(scores)

    study = optuna.create_study(direction="maximize", pruner=MedianPruner(), sampler=TPESampler())

    study.optimize(objective, n_trials=50)

    print(f"The number of trials: {len(study.trials)}")
    print(f"Best value: {study.best_value} (params: {study.best_params})")

    finish_time = time.time()
    print(f"Tuning completed: {finish_time-start_time:.2f} seconds.")

    param = study.best_params
    param["cat_features"] = ['Continent']
    param["random_seed"] = SEED
    param["verbose"] = False
    reg = CatBoostRegressor(**param).fit(X_train_2,y_train)
    y_pred = reg.predict(X_test_2)
    print(r2_score(y_test,y_pred))

residual_plot(reg,X_train_2,y_train,X_test_2,y_test)



"""<a id='shap'></a>
<h2 style="font-family:arial; text-align: justify; border-bottom: 2px solid #c09f80;">Explainability <a href='#contents'>üß†</a></h2>
"""

explainer = shap.TreeExplainer(reg)
data = X_train_2
shap_values = explainer.shap_values(data)
shap.summary_plot(shap_values, data)

i = 0
sample = X_train_2.iloc[[i],:]
shap_values = explainer.shap_values(sample)
# shap.initjs()
shap.force_plot(explainer.expected_value, shap_values, sample,matplotlib=True,figsize=(30, 3), text_rotation=30)

data = X_train_2
shap_values = explainer.shap_values(data)
shap.dependence_plot("HEXP_GDP_ratio", shap_values, data)